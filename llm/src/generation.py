"""
generation.py
--------------
Vastausten generointi LumiOpen/Viking-13B -mallilla.
Painotus: tiukka RAG â€“ vastaa vain lÃ¤hdemateriaalin perusteella.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, logging

logging.set_verbosity_error()


def generate_answer(question: str, context: list[str]):
    """Generoi tiukan, suomenkielisen vastauksen Viking-13B-mallilla."""
    print("\nâš™ï¸ Generoidaan vastaus mallilla LumiOpen/Viking-13B...")

    model_name = "LumiOpen/Viking-13B"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()

    if not context:
        print("âš ï¸ Konteksti on tyhjÃ¤ â€” ei kappaleita, joista vastata.")
        return "En pysty vastaamaan, koska lÃ¤hteitÃ¤ ei lÃ¶ytynyt."

    # ğŸ¯ Tiukka, ohjaava suomenkielinen system prompt
    system_prompt = (
        "Toimit suomenkielisenÃ¤ tekoÃ¤lyavustajana, joka vastaa vain annettujen lÃ¤hteiden perusteella.\n"
        "TehtÃ¤vÃ¤si on kertoa, miten verkkolÃ¤hde merkitÃ¤Ã¤n lÃ¤hdeluetteloon suomalaisessa opinnÃ¤ytetyÃ¶ssÃ¤.\n"
        "KÃ¤ytÃ¤ vain alla annettua kontekstia â€“ Ã¤lÃ¤ keksi omaa sisÃ¤ltÃ¶Ã¤.\n"
        "Jos kontekstissa ei ole ohjetta verkkolÃ¤hteen merkitsemiseen, vastaa: "
        "'En lÃ¶ydÃ¤ varmaa ohjetta annetuista lÃ¤hteistÃ¤.'\n\n"
        "Vastauksesi tulee olla lyhyt (2â€“4 lausetta) ja sisÃ¤ltÃ¤Ã¤ konkreettinen esimerkki muodossa:\n"
        "TekijÃ¤. Vuosi. Otsikko. Verkkosivusto. Saatavilla: URL. Viitattu pp.kk.vvvv.\n"
        "Ã„lÃ¤ lisÃ¤Ã¤ mitÃ¤Ã¤n muuta tekstiÃ¤.\n"
    )

    # ğŸ§© Rakennetaan konteksti â€“ vain olennaisimmat kappaleet
    ctx_text = ""
    for i, p in enumerate(context[:5]):
        if len(tokenizer.encode(ctx_text + p)) > 1500:
            break
        ctx_text += f"[Kappale {i+1}]\n{p}\n\n"

    # ğŸ”¤ Lopullinen prompt
    prompt = (
        f"{system_prompt}"
        f"Kysymys: {question}\n\n"
        f"Alla on lÃ¤hdeaineistosta poimitut kappaleet:\n"
        f"{ctx_text}\n\n"
        "Kirjoita vastaus vain nÃ¤iden kappaleiden pohjalta.\n\nVastaus:"
    )

    # ğŸ§® Tokenointi ja generointi
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(model.device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.4,
            top_p=0.8,
            do_sample=True,
            repetition_penalty=1.15,
        )

    answer = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()

    # ğŸ” Validointi â€” tarkista, ettei malli harhaile
    key_terms = ["lÃ¤hdeluettelo", "verkkolÃ¤hde", "viitattu", "Saatavilla"]
    if not any(k in answer.lower() for k in key_terms):
        print("âš ï¸ Mallin vastaus ei sisÃ¤ltÃ¤nyt aiheeseen liittyviÃ¤ avainsanoja â€” yritetÃ¤Ã¤n uudelleen vÃ¤hemmÃ¤llÃ¤ lÃ¤mmÃ¶llÃ¤.")
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=250,
                temperature=0.2,
                top_p=0.7,
                do_sample=True,
                repetition_penalty=1.2,
            )
        answer = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()

    # ğŸ§¹ Puhdistetaan lopputulos
    for unwanted in ["\n\n", "\n", "###", "Vastaus:", "LÃ¤hteet:", "Kysymys:"]:
        if answer.startswith(unwanted):
            answer = answer.replace(unwanted, "").strip()

    print("\nğŸ“ Generoitu vastaus valmiina.\n")
    return answer
